{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS475: Programming Assignment 2.1:--> Understanding Tokenization\n",
    "\n",
    "\n",
    "In this homework, you will build implement some basic tokenization algorithms to understand tokenization better. \n",
    "\n",
    "Part of this homework's design has been inspired from Assignment 1 of UW CSE447. \n",
    "\n",
    "IMPORTANT: Use python 3.7 or later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to submit**\n",
    "* Fill out <mark>TODO</mark> blocks, **DO NOT** modify other parts of the skeleton code.\n",
    "* Submit one file: hw2_{student_ID}.ipynb to KLMS\n",
    "    e.g.hw2_20243150.ipynb\n",
    "* **Late submission policy**: After the submission deadline, you will immediately lose 20% of the score, another 20% after 24 hours later, and so on.\n",
    "Submission after 72 hours (3 days) will not be counted. However, you can use late days for this assignment, for which you have to send an email to inform the TAs. See course syllabus website Late Policy section. Use this email for this purpose: kaistcs475@gmail.com\n",
    "\n",
    "**Note**\n",
    "* Make a copy of this .ipynb file. Do not directly edit this file.\n",
    "* You are required to use numpy, do not use neither pytorch nor tensorflow.\n",
    "* Check whether your whole cells work well by restarting runtime code and running all before the submission.\n",
    "* TA will look into the implemented functions, their validity and give corresponding score to each <mark>TODO</mark> problem.\n",
    "* Make reasonable assumptions when needed, with justification provided. \n",
    "* You can reuse code segments from earlier sections to fill up later sections.\n",
    "* Ask questions through Slack so that you can share information with other students.\n",
    "* TA in charge: Sheikh Shafayat (sheikh.shafayat@kaist.ac.kr)\n",
    "\n",
    "**For Help** \n",
    "If you are having trouble understanding the the homework, watch [this](https://youtu.be/kCc8FmEb1nY?si=3wiq6ETKJ4g2LP0a) video end to end to gain a deep understanding of tokenizers. However, note that this homework is not built on top of that codebase and in fact even the naming conventions are different here. Use the video just for understanding the concepts well. \n",
    "\n",
    "If you are short on time, reviewing the TA session slides on Tokenizers will also be helpful to solve the homework (all the images here are taken from there)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use python 3.7 or later (IMPORTANT)\n",
    "\n",
    "import numpy as np \n",
    "from collections import Counter\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "\n",
    "# set seed\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Jane Austen's \"Emma\" novel as the running example for the rest of our homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "with open(\"./Emma.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 899644 characters\n",
      "number of unique characthers 92\n"
     ]
    }
   ],
   "source": [
    "# see dataset properties:\n",
    "print(f\"Total: {len(list(text))} characters\")\n",
    "print(f\"number of unique characthers {len(set(text))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the Emma dataset is quite big. So, we will use this small dataset from [NYTimes](https://www.nytimes.com/2024/09/04/opinion/yuval-harari-ai-democracy.html) for debugging purpose. Use this only for debugging and not for reporting final results. Do your experiments with Emma dataset and the data kept in the multilingual-data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ = f\"\"\"Democracy is a conversation. Its function and survival depend on the available information technology. For most of history, no technology existed for holding large-scale conversations among millions of people. In the premodern world, democracies existed only in small city-states like Rome and Athens, or in even smaller tribes. Once a polity grew large, the democratic conversation collapsed, and authoritarianism remained the only alternative.\n",
    "\n",
    "Large-scale democracies became feasible only after the rise of modern information technologies like the newspaper, the telegraph and the radio. The fact that modern democracy has been built on top of modern information technologies means that any major change in the underlying technology is likely to result in a political upheaval.\n",
    "\n",
    "This partly explains the current worldwide crisis of democracy. In the United States, Democrats and Republicans can hardly agree on even the most basic facts, such as who won the 2020 presidential election. A similar breakdown is happening in numerous other democracies around the world, from Brazil to Israel and from France to the Philippines.\n",
    "\n",
    "In the early days of the internet and social media, tech enthusiasts promised they would spread truth, topple tyrants and ensure the universal triumph of liberty. So far, they seem to have had the opposite effect. We now have the most sophisticated information technology in history, but we are losing the ability to talk with one another, and even more so the ability to listen.\n",
    "\n",
    "As technology has made it easier than ever to spread information, attention became a scarce resource, and the ensuing battle for attention resulted in a deluge of toxic information. But the battle lines are now shifting from attention to intimacy. The new generative artificial intelligence is capable of not only producing texts, images and videos, but also conversing with us directly, pretending to be human.\n",
    "\n",
    "Over the past two decades, algorithms fought algorithms to grab attention by manipulating conversations and content. In particular, algorithms tasked with maximizing user engagement discovered by experimenting on millions of human guinea pigs that if you press the greed, hate or fear button in the brain, you grab the attention of that human and keep that person glued to the screen. The algorithms began to deliberately promote such content. But the algorithms had only limited capacity to produce this content by themselves or to directly hold an intimate conversation. This is now changing, with the introduction of generative A.I.s like OpenAI’s GPT-4.\n",
    "\"\"\"\n",
    "\n",
    "# The author(s) of this homework do not necessarily agree/endorse the views expressed in the text above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary: Tokenization\n",
    "\n",
    "Before you start: go [here](https://tiktokenizer.vercel.app/) and play with several tokenizers. Specially play with cl100k_base (GPT4) and GPT2 tokenizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TO DO #### \n",
    "\n",
    "# Write down one thing that surprised you while playing with the tokenizer visualzier above (in 2-3 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's start with implementing the simplest form of the tokenizer: character level tokenizer. In fact many early NLP research were conducted using character level tokenization. \n",
    "\n",
    "Notice the CharTokenizer class below. For the rest of the homework, your implementation will follow a similar structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTokenizer: \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        All of your tokenizer implementation should have tokenizer_map attribute\n",
    "        tokenizer_map is a dictionary that maps individual numbers to corresponding tokens\n",
    "        map_of_char maps individual characters to corresponding token indices. In this particular case, it is the reverse of tokenizer_map; however, it is not always the case. map_of_char is an optional attribute for your tokenizer implementation. You may not need it for your implementation.\n",
    "        \"\"\"\n",
    "        self.tokenizer_map = None \n",
    "        self.map_of_char = None\n",
    "        \n",
    "\n",
    "    def train(self, text):\n",
    "\n",
    "        self.character_set = set(text)\n",
    "        self.tokenizer_map = {i: char for i, char in enumerate(self.character_set)}\n",
    "        self.map_of_char = {char: i for i, char in self.tokenizer_map.items()}\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        return [self.map_of_char[char] for char in text]\n",
    "        \n",
    "    def decode_text(self, encoded_text):\n",
    "        return \"\".join([list(self.map_of_char.keys())[i] for i in encoded_text])\n",
    "\n",
    "chat_tokenizer = CharTokenizer()\n",
    "chat_tokenizer.train(text_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now see the usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text:\n",
      "\n",
      "Democracy is a conversation. Its function and survival depend on the available information technology. For most of history, no technology existed for holding large-scale conversations among millions of people. In the premodern world, democracies existed only in small city-states like Rome and Athens, or in even smaller tribes. Once a polity grew large, the democratic conversation collapsed, and authoritarianism remained the only alternative.\n",
      "\n",
      "Large-scale democracies became feasible only after the rise of modern information technologies like the newspaper, the telegraph and the radio. The fact that modern democracy has been built on top of modern information technologies means that any major change in the underlying technology is likely to result in a political upheaval.\n",
      "\n",
      "This partly explains the current worldwide crisis of democracy. In the United States, Democrats and Republicans can hardly agree on even the most basic facts, such as who won the 2020 presidential election. A similar breakdown is happening in numerous other democracies around the world, from Brazil to Israel and from France to the Philippines.\n",
      "\n",
      "In the early days of the internet and social media, tech enthusiasts promised they would spread truth, topple tyrants and ensure the universal triumph of liberty. So far, they seem to have had the opposite effect. We now have the most sophisticated information technology in history, but we are losing the ability to talk with one another, and even more so the ability to listen.\n",
      "\n",
      "As technology has made it easier than ever to spread information, attention became a scarce resource, and the ensuing battle for attention resulted in a deluge of toxic information. But the battle lines are now shifting from attention to intimacy. The new generative artificial intelligence is capable of not only producing texts, images and videos, but also conversing with us directly, pretending to be human.\n",
      "\n",
      "Over the past two decades, algorithms fought algorithms to grab attention by manipulating conversations and content. In particular, algorithms tasked with maximizing user engagement discovered by experimenting on millions of human guinea pigs that if you press the greed, hate or fear button in the brain, you grab the attention of that human and keep that person glued to the screen. The algorithms began to deliberately promote such content. But the algorithms had only limited capacity to produce this content by themselves or to directly hold an intimate conversation. This is now changing, with the introduction of generative A.I.s like OpenAI’s GPT-4.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, initialize the tokenizer\n",
    "char_tokenizer = CharTokenizer()\n",
    "\n",
    "# Then train the tokenizer on the Emma text\n",
    "char_tokenizer.train(text) \n",
    "\n",
    "# Now encode some texts using the trained tokenizer\n",
    "enc = char_tokenizer.encode_text(text_) # notice that we are using text_ here\n",
    "\n",
    "# Now decode the encoded text\n",
    "print(f\"Decoded text:\\n\\n{char_tokenizer.decode_text(enc)}\")\n",
    "\n",
    "# If the decoded text is the same as the original text, then the implementation is correct\n",
    "# use this sanity check to verify all of your implementations\n",
    "assert char_tokenizer.decode_text(char_tokenizer.encode_text(text_)) == text_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in reality, we do not use character level tokenization. What we use is subword level tokenization. This is why you will implement one of the most effective and popular subword level tokenization method called BPE algorithm. \n",
    "\n",
    "\n",
    "First, you will implement the vanilla version of it.\n",
    "\n",
    "## 1. Vanilla BPETokenizer: \n",
    "This version does *not* have a preprocessing step. This is also called SentencePiece tokenization (do not confuse with the library with the same name) and was introduced into [this paper](https://arxiv.org/pdf/1808.06226)\n",
    "Check out this pseudocode for a clearer understanding:\n",
    "\n",
    "![Vanilla BPETokenizer](./images/vanilla%20BPE.png)\n",
    "\n",
    "Note that when multiple bigrams have same frequency, ties can be broken by picking a random one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer: \n",
    "    def __init__(self):\n",
    "        \"\"\" \n",
    "        READ THIS DESCRIPTION:\n",
    "\n",
    "        map_of_merged_tokens: is a dictionary that maps the merged bigram to a single token. According to the notations in the pseudocode above, it is a dictionary with keys (v_i, v_j) and values v_n. You need to keep this dictionary updated as you merge tokens in the training step. This dictionary is needed when decoding. When done right, it should look something like {(5, 68): 92, (68, 23): 93, (68, 35): 94, ...}. Here it means that token 5 and 68 were merged to form token 92, 68 and 23 were merged to form token 93, and so on. *The order in the dictionary refers to the order of merging.* \n",
    "        tokenizer_map: is a dictionary that maps token numbers to their corresponding tokens (subwords). Its length should be equal to the target_vocab_size. After a successful training, tokenizer_map might look something like {..., '995: 'uten',996: 'ion of ', 997: ' turn', ...} Notice that this behavior is opposite to get_vocab method in huggingface tokenizers, where the subwords are the keys and the values are the token numbers.\n",
    "        map_of_char: maps individual characters to corresponding token indices. Optional. Use it only if your implementation needs.\n",
    "        \"\"\"\n",
    "        self.map_of_merged_tokens = None\n",
    "        self.tokenizer_map = None\n",
    "        self.map_of_char = None\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_pairs(lst:list, bigram:tuple, c: int):\n",
    "\n",
    "        \"\"\" \n",
    "        Helper function to replace bigram with a single token in the list. \n",
    "        Whether to use this particular function or not depends on *your* implementations. \n",
    "        It is possible that you may not need this function at all.\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        while i < len(lst) - 1:\n",
    "            if lst[i] == bigram[0] and lst[i + 1] == bigram[1]:\n",
    "                lst[i] = c\n",
    "                del lst[i + 1]\n",
    "            else:\n",
    "                i += 1\n",
    "        return lst\n",
    "\n",
    "    def train(self, text: str, target_vocab_size: int):\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        character_set = set(text)\n",
    "        vocabsize = len(character_set) # initial vocabulary size based on the number of unique characters in the text\n",
    "        assert self.target_vocab_size > vocabsize, \"target vocab size must be greater than the number of unique characters in the text\"\n",
    "        \n",
    "        map_of_char = {char: i for i, char in enumerate(character_set)}\n",
    "        self.map_of_char = map_of_char\n",
    "        \n",
    "        map_of_merged_tokens = {}\n",
    "        tokenized = [map_of_char[char] for char in text] # initial tokenization based on the characters\n",
    "\n",
    "        #### TO DO: Write the loop and finish the implementation ####\n",
    "\n",
    "      \n",
    "        #### END ####\n",
    "        self.map_of_merged_tokens = map_of_merged_tokens\n",
    "        self.tokenizer_map = # set this to the tokenizer_map you created in the loop above\n",
    "\n",
    "    \n",
    "    def encode_text(self, text: str):\n",
    "        \"\"\"Given a text, return the tokenized text as a list of integers\"\"\"\n",
    "\n",
    "        ### TO DO: Write the implementation of the encode_text function ###\n",
    "        \n",
    "        ### END ###\n",
    "        return # tokenized\n",
    "    \n",
    "    def decode_text(self, tokenized_text: list): \n",
    "        \"\"\"Given a tokenized text, return the original text as a string\"\"\"\n",
    "        ### TO DO: Write the implementation of the decode_text function ###\n",
    "\n",
    "        ### END ###\n",
    "        return # decoded\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training BPE Tokenizer: 100%|██████████| 52/52 [00:00<00:00, 1526.27it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BPETokenizer()\n",
    "\n",
    "tokenizer.train(text_, 100) # just for checking if the training is running or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Life is beautiful'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text = tokenizer.encode_text(\"Life is beautiful\")\n",
    "tokenizer.decode_text(encoded_text)\n",
    "\n",
    "assert tokenizer.decode_text(tokenizer.encode_text(\"Life is beautiful\")) == \"Life is beautiful\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. BPE Tokenizer with preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in reality we apply a preprocessing step before the BPE tokenization pipeline. \n",
    "\n",
    "In the preprocessing step, we split the text with a complex regex pattern to separate the texts on the space and punctuation boundary. \n",
    "\n",
    "To see the effect of the regex, see this example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this code: \n",
    "\n",
    "# re.findall(r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\", text_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the whole dataset into small chunks, we apply the algorithm on each chunk, that is, when we count the bigrams, we do not count bigrams from two neigboring chunks. See the pseudocode below:\n",
    "![BPETokenizer Preprocess](./images/BPE_preprocess.png)\n",
    "\n",
    "However, note that here the example shows everything on \"text\" space (\"hug\", \"pug\", \"i\" etc) for the sake of interpretability. In reality, we do these merging on the token space (v1, v2, v3 ...), ie, on the token ids.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizerwithPreprocessing: \n",
    "    def __init__(self):\n",
    "        self.target_vocab_size = None\n",
    "        self.map_of_merged_tokens = None\n",
    "        self.tokenizer_map = None\n",
    "        self.map_of_char = None\n",
    "        self.split_pattern = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "        # this is the split boundary pattern used in GPT4 tokenizer. Feel free to ask ChatGPT to explain what this regex pattern does.\n",
    "    \n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def replace_pairs(lst:list, bigram:tuple, c: int):\n",
    "\n",
    "        \"\"\" \n",
    "        Helper function to replace bigram with a single token in the list. \n",
    "        Whether to use this particular function or not depends on *your* implementation of the tokenizer. \n",
    "        It is possible that you may not need this function at all.\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        while i < len(lst) - 1:\n",
    "            if lst[i] == bigram[0] and lst[i + 1] == bigram[1]:\n",
    "                lst[i] = c\n",
    "                del lst[i + 1]\n",
    "            else:\n",
    "                i += 1\n",
    "        return lst\n",
    "    \n",
    "\n",
    "    ### OPTIONAL (not graded) ####\n",
    "\n",
    "    # If you need, you can define another helper function to replace bigrams in the list of lists.\n",
    "        \n",
    "    \n",
    "    #### END ####\n",
    "    def preprocess_text(self, text: str):\n",
    "        return re.findall(self.split_pattern, text)\n",
    "    \n",
    "    def train(self, text: str, target_vocab_size: int):\n",
    "        \n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        character_set = set(text)\n",
    "        vocabsize = len(character_set)\n",
    "        map_of_char = {char: i for i, char in enumerate(character_set)}\n",
    "        self.map_of_char = map_of_char\n",
    "        map_of_merged_tokens = {}\n",
    "        print(f\"Preprocessing the text\")\n",
    "        preprocessed = self.preprocess_text(text)\n",
    "\n",
    "        #### TO DO: Write the loop and finish the implementation ####\n",
    "\n",
    "       \n",
    "\n",
    "        #### END ####\n",
    "        self.map_of_merged_tokens = map_of_merged_tokens\n",
    "        self.tokenizer_map = # set this to the tokenizer_map you created in the loop above\n",
    "        \n",
    "\n",
    "    def encode_text(self, text: str):\n",
    "        \"\"\"Given a text, return the tokenized text\"\"\"\n",
    "        ### TO DO: Write the implementation of the encode_text function ###\n",
    "\n",
    "        \n",
    "        #### END ####\n",
    "        return tokenized\n",
    "    \n",
    "    def decode_text(self, tokenized_text: list): \n",
    "        \"\"\"Given a tokenized text, return the original text\"\"\"\n",
    "        ### TO DO: Write the implementation of the encode_text function ###\n",
    "\n",
    "        #### END ####\n",
    "        return decode_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing the text\n",
      "Initial vocab size: 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training BPE Tokenizer: 100%|██████████| 408/408 [00:55<00:00,  7.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Life is beautiful'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BPETokenizerwithPreprocessing()\n",
    "tokenizer.train(text, 500) \n",
    "encoded = tokenizer.encode_text(\"Life is beautiful\")\n",
    "tokenizer.decode_text(encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded = tokenizer.encode_text(\"인생은 아름답다\") # if you try Korean text, it will raise an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing in bytespace "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only worked with English texts. \n",
    "\n",
    "But of course, there are thousands of other languages in the world, and not all languages can be represented in the way English is represented so far. This is more true for character based langauges like Chinese. Even for Korean language, building from the character level [like the way we did in English] is not possible. Which is why we need an universal way to represent languages of various scripts. Utf-8 provides such a way. \n",
    "\n",
    "Utf-8 is an encoding method that encodes **all** human written languages into variable length byte sequece of up to 4 bytes (256 bits). One way to think of it is to have an universal \"alphabet\" for all human written language, where the alphabet size is 256. \n",
    "\n",
    "However, the alphabet analogy is not quite right. To see why, see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life is encoded as: [76, 105, 102, 101]\n",
      "L is encoded as: [76]\n",
      "i is encoded as: [105]\n",
      "f is encoded as: [102]\n",
      "e is encoded as: [101]\n",
      "생일 is encoded as: [236, 131, 157, 236, 157, 188]\n",
      "생 is encoded as: [236, 131, 157]\n",
      "ㅅ is encoded as: [227, 133, 133]\n",
      "새 is encoded as: [236, 131, 136]\n",
      "이 is encoded as: [236, 157, 180]\n",
      "일 is encoded as: [236, 157, 188]\n",
      "🎂 is encoded as: [240, 159, 142, 130]\n",
      "🤞 is encoded as: [240, 159, 164, 158]\n"
     ]
    }
   ],
   "source": [
    "# Let's play with uft-8 encoding\n",
    "s = \"Life\"\n",
    "print(f\"{s} is encoded as: {list(s.encode('utf-8'))}\")\n",
    "print(f\"L is encoded as: {list('L'.encode('utf-8'))}\")\n",
    "print(f\"i is encoded as: {list('i'.encode('utf-8'))}\")\n",
    "print(f\"f is encoded as: {list('f'.encode('utf-8'))}\")\n",
    "print(f\"e is encoded as: {list('e'.encode('utf-8'))}\")\n",
    "\n",
    "\n",
    "s = \"생일\"\n",
    "print(f\"{s} is encoded as: {list(s.encode('utf-8'))}\")\n",
    "print(f\"생 is encoded as: {list('생'.encode('utf-8'))}\")\n",
    "print(f\"ㅅ is encoded as: {list('ㅅ'.encode('utf-8'))}\")\n",
    "print(f\"새 is encoded as: {list('새'.encode('utf-8'))}\")\n",
    "print(f\"이 is encoded as: {list('이'.encode('utf-8'))}\")\n",
    "print(f\"일 is encoded as: {list('일'.encode('utf-8'))}\")\n",
    "\n",
    "s = \"🎂\"\n",
    "print(f\"{s} is encoded as: {list(s.encode('utf-8'))}\")\n",
    "\n",
    "s= \"🤞\"\n",
    "print(f\"{s} is encoded as: {list(s.encode('utf-8'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you see that utf-8 can also encode emojies and many languages? However, also note that the mapping from each alphabet to utf-8 encoding is not always very intuitive (look at the Korean example carefully). The reason English alphabet takes up lower values than non-English alphabet is because utf-8 is backward compatible (ASCII). For all non English languages, the encoded lists are usually long. \n",
    "\n",
    "Let's look at some more examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded: 생일\n",
      "Decoded: ���일\n"
     ]
    }
   ],
   "source": [
    "# decoding \n",
    "encoded = [236, 131, 157, 236, 157, 188]\n",
    "print(f\"Decoded: {bytes(encoded).decode('utf-8', errors='replace')}\")\n",
    "\n",
    "encoded = [150, 131, 157, 236, 157, 188] \n",
    "print(f\"Decoded: {bytes(encoded).decode('utf-8', errors='replace')}\")\n",
    "\n",
    "# the second one is not a valid utf-8 encoding, so it will be replaced with a question mark \n",
    "# if your language model is not strong enough in modeling lanuguages, it will through more errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know more about utf-8 encoding, check out the corresponding Wikipedia page. \n",
    "\n",
    "Because of the obvious advantage of using utf-8 encoding, when we train a real encoder, we first project all the data into byte-space and then do the tokenizer training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ByteSpacePreprocessed\n",
    "BPE algorithm on byte space, however, the text first go through the regex preprocessing. Make sure to do the preprocessing first. \n",
    "\n",
    "Make sure to do the preprocessing **before** the byte encoding (and not after)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizerByteSpacePreprocessed: \n",
    "    def __init__(self):\n",
    "\n",
    "        self.map_of_merged_tokens = None\n",
    "        self.tokenizer_map = None\n",
    "        self.map_of_char = None\n",
    "        self.split_pattern = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "\n",
    "    ### OPTIONAL (not graded) ####\n",
    "\n",
    "    # If you need, you can define another helper function to replace bigrams in the list of lists.\n",
    "    \n",
    "    \n",
    "    #### END ####\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_pairs(lst:list, bigram:tuple, c: int):\n",
    "\n",
    "        \"\"\" \n",
    "        Helper function to replace bigram with a single token in the list. \n",
    "        Whether to use this particular function or not depends on *your* implementation of the tokenizer. \n",
    "        It is possible that you may not need this function at all.\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        while i < len(lst) - 1:\n",
    "            if lst[i] == bigram[0] and lst[i + 1] == bigram[1]:\n",
    "                lst[i] = c\n",
    "                del lst[i + 1]\n",
    "            else:\n",
    "                i += 1\n",
    "        return lst\n",
    "    \n",
    "    def preprocess_text(self, text: str):\n",
    "        return re.findall(self.split_pattern, text)\n",
    "\n",
    "    def train(self, text: str, target_vocab_size: int):\n",
    "\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "\n",
    "        \n",
    "\n",
    "        character_set = set([i for i in range(256)]) # We start with all the possible bytes, which is 256\n",
    "        vocabsize = len(character_set)\n",
    "        assert self.target_vocab_size > vocabsize, \"target vocab size must be greater than 256\"\n",
    "\n",
    "        map_of_char = {char: i for i, char in enumerate(character_set)}\n",
    "        self.map_of_char = map_of_char\n",
    "        \n",
    "        map_of_merged_tokens = {}\n",
    "        ### Implement the tokenizer training in byte space. Make sure to preprocess the texts first####\n",
    "        # Your implementation should not be too different from what you have done before. \n",
    "        # Just make sure to deal with the quirks of working with bytes instead of characters.\n",
    "        \n",
    "        \n",
    "\n",
    "        ### END ###\n",
    "\n",
    "        self.map_of_merged_tokens = map_of_merged_tokens\n",
    "        self.tokenizer_map = # set this to the tokenizer_map you created in the loop above\n",
    "\n",
    "    \n",
    "    def encode_text(self, text: str):\n",
    "        \"\"\"Given a text, return the tokenized text\"\"\"\n",
    "        ### Implement the encode_text function ###\n",
    "        # You only need to sligthly modify existing code in your previous implementation \n",
    "        \n",
    "        ### END ###\n",
    "        return tokenized\n",
    "    \n",
    "    def decode_text(self, tokenized_text: list): \n",
    "        \"\"\"Given a tokenized text, return the original text\"\"\"\n",
    "\n",
    "        ### TO DO: Write the implementation of the decode_text function ###\n",
    "        # make sure to use errors='replace' when decoding\n",
    "        \n",
    "        return decode_text\n",
    "        \n",
    "    \n",
    "        ### END ###\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training BPE Tokenizer: 100%|██████████| 244/244 [00:01<00:00, 163.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Life is beautiful 🤞'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "\n",
    "with open(\"./kor.txt\", \"r\") as f: # grab some Korean text from Wikipedia\n",
    "    kor_text = f.read()\n",
    "\n",
    "tokenizer = BPETokenizerByteSpacePreprocessed()\n",
    "tokenizer.train(kor_text, 500)  # train the tokenizer on the Korean text\n",
    "encoded = tokenizer.encode_text(\"Life is beautiful 🤞\") # still manages to tokenize English\n",
    "decoded = tokenizer.decode_text(encoded) \n",
    "\n",
    "print(f\"Decoded: {decoded}\")\n",
    "\n",
    "assert decoded == \"Life is beautiful 🤞\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with Korean text; \n",
    "\n",
    "encoded = tokenizer.encode_text(\"생일 축하해\")\n",
    "decoded = tokenizer.decode_text(encoded)\n",
    "\n",
    "assert decoded == \"생일 축하해\" \n",
    "# and of course it works with Korean as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO ###\n",
    "\n",
    "# What are some distinctions you notice between the tokens produced by byte-level BPE with preprocessing vs vanilla BPE?\n",
    "\n",
    "# Train tokenizers using the Emma text first and do some analysis\n",
    "# Feel free to run some codes and show some examples, plot graphs etc.\n",
    "# citing papers/other resources is also fine, but make sure to explain the distinctions in your own words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual Tokenization analysis: \n",
    "\n",
    "You are given two files in the ./multilingual-data folder. The English one is a chunk of the original English Emma novel. The Korean one is machine translated version of that. Now you will think about some properties of the tokenizers when trained on different corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./multilingual-data/kor_Emma.txt\", \"r\") as f: \n",
    "    kor_emma = f.read()\n",
    "\n",
    "with open(\"./multilingual-data/en_Emma.txt\", \"r\") as f: \n",
    "    eng_emma = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_text_en = \"\"\"“Ever since the day—about four years ago—that Miss Taylor and I met\n",
    "with him in Broadway Lane, when, because it began to drizzle, he darted\n",
    "away with so much gallantry, and borrowed two umbrellas for us from\n",
    "Farmer Mitchell’s, I made up my mind on the subject. I planned the\n",
    "match from that hour; and when such success has blessed me in this\n",
    "instance, dear papa, you cannot think that I shall leave off\n",
    "match-making.”\n",
    "\n",
    "“I do not understand what you mean by ‘success,’” said Mr. Knightley.\n",
    "“Success supposes endeavour. Your time has been properly and delicately\n",
    "spent, if you have been endeavouring for the last four years to bring\n",
    "about this marriage. A worthy employment for a young lady’s mind! But\n",
    "if, which I rather imagine, your making the match, as you call it,\n",
    "means only your planning it, your saying to yourself one idle day, ‘I\n",
    "think it would be a very good thing for Miss Taylor if Mr. Weston were\n",
    "to marry her,’ and saying it again to yourself every now and then\n",
    "afterwards, why do you talk of success? Where is your merit? What are\n",
    "you proud of? You made a lucky guess; and _that_ is all that can be\n",
    "said.”\n",
    "\n",
    "“And have you never known the pleasure and triumph of a lucky guess?—I\n",
    "pity you.—I thought you cleverer—for, depend upon it a lucky guess is\n",
    "never merely luck. There is always some talent in it. And as to my poor\n",
    "word ‘success,’ which you quarrel with, I do not know that I am so\n",
    "entirely without any claim to it. You have drawn two pretty pictures;\n",
    "but I think there may be a third—a something between the do-nothing and\n",
    "the do-all. If I had not promoted Mr. Weston’s visits here, and given\n",
    "many little encouragements, and smoothed many little matters, it might\n",
    "not have come to any thing after all. I think you must know Hartfield\n",
    "enough to comprehend that.”\"\"\"\n",
    "\n",
    "eval_text_kor = \"\"\"\"4년 전쯤 테일러 양과 내가 브로드웨이 레인에서 그를 만난 날부터, 이슬비가 내리기 시작하자 그는 매우 용감하게 달려가서\n",
    "Farmer Mitchell's에서 우산 두 개를 빌렸을 때부터, 나는 그 문제에 대해 마음먹었습니다. 나는 그 순간부터\n",
    "결혼을 계획했습니다. 그리고 이런 성공이 이 경우에 나에게 축복이 되었을 때, 사랑하는 아빠, 당신은 내가\n",
    "결혼을 그만둘 것이라고 생각할 수 없습니다.\"\n",
    "\n",
    "\"나는 당신이 '성공'이라는 말로 무슨 뜻인지 이해하지 못합니다.\" 나이틀리 씨가 말했습니다.\n",
    "\"성공은 노력을 전제로 합니다. 지난 4년 동안 이 결혼을 이루기 위해 노력했다면, 당신의 시간은 적절하고 신중하게\n",
    "보내졌습니다. 젊은 여성의 마음에 가치 있는 일이었습니다! 하지만\n",
    "만약 당신이 결혼을 한다고 상상한다면, 당신이 말하는 대로\n",
    "그저 계획하는 것일 뿐이고, 어느 날 한가한 시간에 ‘웨스턴 씨가 테일러 양과 결혼하면 아주 좋은 일이 될 것 같아’라고 스스로에게 말하고, 그 후로 가끔씩 스스로에게 다시 말하는 것일 뿐이라면, 왜 성공에 대해 이야기하는 거지? 당신의 공로는 어디에 있지? 당신은 무엇을 자랑스러워하는 거지? 당신은 행운의 추측을 했고; _그게_ 말할 수 있는 전부야.\n",
    "\n",
    "\"그리고 당신은 행운의 추측의 즐거움과 승리를 결코 알지 못했니?—나는 당신을\n",
    "불쌍히 여긴다.—나는 당신이 더 똑똑하다고 생각했다—왜냐하면, 행운의 추측은\n",
    "결코 단순한 행운이 아니기 때문이다. 항상 어떤 재능이 거기에 있다. 그리고 당신이 다투는 나의 형편없는\n",
    "단어 ‘성공’에 대해 말하자면, 나는 내가 그것에 대한 권리가 전혀 없다는 것을 모른다. 당신은 두 개의 예쁜 그림을 그렸다.\n",
    "하지만 나는 세 번째 그림이 있을 수 있다고 생각한다—아무것도 하지 않는 것과\n",
    "모든 것을 하는 것 사이의 무언가. 내가 웨스턴 씨의 방문을 홍보하지 않았고,\n",
    "많은 작은 격려를 하지 않았고, 많은 작은 문제들을 해결하지 않았다면,\n",
    "결국 아무것도 이루어지지 않았을지도 모릅니다. 당신은 하트필드를\n",
    "충분히 알고 있을 테니 그걸 이해할 수 있을 겁니다.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Open Ended \n",
    "\n",
    "# Play around with the BPETokenizerByteSpacePreprocessed that you implemented using the Korean and English Emma data, which you can use for training the tokenizers, and write down if you see anything interesting. \n",
    "\n",
    "# What does your findings imply for designing multilingual tokenizers? \n",
    "\n",
    "# Feel free to nudge vocabulary size as well. You can use the eval_text in the above cell for validation.\n",
    "\n",
    "# Do not hesitate to look into the actual tokens and see how much of it makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordpiece \n",
    "\n",
    "Now, implement the wordpiece tokenizer algorithm, which is almost similar to BPE, except the objective function is a bit different. \n",
    "\n",
    "Check the description below. \n",
    "\n",
    "![Wordpiece Algorithm](./images/Wordpiece.png)\n",
    "\n",
    "\n",
    "Make sure to:\n",
    "1. Preprocess the text first\n",
    "2. Work on the byte-space (utf-8 encoding)\n",
    "3. When you estimate p(v_i, v_j), divide by N-1, instead of N where N is the total number of tokens in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPieceTokenizer: \n",
    "    def __init__(self):\n",
    "\n",
    "        self.map_of_merged_tokens = None\n",
    "        self.tokenizer_map = None\n",
    "        self.map_of_char = None\n",
    "        self.split_pattern = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "    \n",
    "    ### OPTIONAL ####\n",
    "    # You can define another helper function to replace bigrams in the list of lists.\n",
    "\n",
    "    \n",
    "    #### END ####\n",
    "    \n",
    "    @staticmethod\n",
    "    def replace_pairs(lst:list, bigram:tuple, c: int):\n",
    "\n",
    "        \"\"\" \n",
    "        Helper function to replace bigram with a single token in the list. \n",
    "        Whether to use this particular function or not depends on *your* implementation of the tokenizer. \n",
    "        It is possible that you may not need this function at all.\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        while i < len(lst) - 1:\n",
    "            if lst[i] == bigram[0] and lst[i + 1] == bigram[1]:\n",
    "                lst[i] = c\n",
    "                del lst[i + 1]\n",
    "            else:\n",
    "                i += 1\n",
    "        return lst\n",
    "    \n",
    "    def preprocess_text(self, text: str):\n",
    "        return re.findall(self.split_pattern, text)\n",
    "\n",
    "    def train(self, text: str, target_vocab_size: int):\n",
    "\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "\n",
    "        \n",
    "\n",
    "        character_set = set([i for i in range(256)]) # We start with all the possible bytes, which is 256\n",
    "        vocabsize = len(character_set)\n",
    "        assert self.target_vocab_size > vocabsize, \"target vocab size must be greater than 256\"\n",
    "\n",
    "        map_of_char = {char: i for i, char in enumerate(character_set)}\n",
    "        self.map_of_char = map_of_char\n",
    "        \n",
    "        map_of_merged_tokens = {}\n",
    "\n",
    "        ### TODO: Your implementation of wordpiece training goes here. ### \n",
    "        # Make sure to preprocess the text first\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "        ### END ###\n",
    "\n",
    "        self.map_of_merged_tokens = map_of_merged_tokens\n",
    "        self.tokenizer_map = # set this to the tokenizer_map you created in the loop above\n",
    "\n",
    "    \n",
    "    def encode_text(self, text: str):\n",
    "        \"\"\"Given a text, return the tokenized text\"\"\"\n",
    "        ### Implement the encode_text function ###\n",
    "        # You only need to sligthly modify existing code in your previous implementation \n",
    "        \n",
    "\n",
    "        return tokenized\n",
    "    \n",
    "    def decode_text(self, tokenized_text: list): \n",
    "        \"\"\"Given a tokenized text, return the original text\"\"\"\n",
    "\n",
    "        ### TO DO: Write the implementation of the decode_text function ###\n",
    "        return decoded\n",
    "        # make sure to use errors='replace' when decoding\n",
    "    \n",
    "        ### END ###\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "tokenizer = WordPieceTokenizer()\n",
    "tokenizer.train(text, 500)\n",
    "encoded = tokenizer.encode_text(\"Life is beautiful 🤞\")\n",
    "decoded = tokenizer.decode_text(encoded)\n",
    "\n",
    "assert decoded == \"Life is beautiful 🤞\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimlhw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
